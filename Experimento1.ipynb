{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\proyecto-transfer-learning\\Config\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'AutoencoderU.AutoEncoder'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miaredescnncnn\u001b[0m (\u001b[33miaredescnncnn-instituto-tecnol-gico-de-costa-rica\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20241115_160912-n89vr6pe</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/iaredescnncnn-instituto-tecnol-gico-de-costa-rica/project-three-autoencoder/runs/n89vr6pe' target=\"_blank\">earnest-armadillo-12</a></strong> to <a href='https://wandb.ai/iaredescnncnn-instituto-tecnol-gico-de-costa-rica/project-three-autoencoder' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/iaredescnncnn-instituto-tecnol-gico-de-costa-rica/project-three-autoencoder' target=\"_blank\">https://wandb.ai/iaredescnncnn-instituto-tecnol-gico-de-costa-rica/project-three-autoencoder</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/iaredescnncnn-instituto-tecnol-gico-de-costa-rica/project-three-autoencoder/runs/n89vr6pe' target=\"_blank\">https://wandb.ai/iaredescnncnn-instituto-tecnol-gico-de-costa-rica/project-three-autoencoder/runs/n89vr6pe</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type    | Params | Mode \n",
      "--------------------------------------------\n",
      "0 | encoder | Encoder | 4.7 M  | train\n",
      "1 | decoder | Decoder | 3.0 M  | train\n",
      "2 | loss_fn | MSELoss | 0      | train\n",
      "--------------------------------------------\n",
      "7.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.7 M     Total params\n",
      "30.790    Total estimated model params size (MB)\n",
      "42        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:419: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:419: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n",
      "c:\\ProgramData\\miniconda3\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:298: The number of training batches (36) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 36/36 [30:21<00:00,  0.02it/s, v_num=r6pe, autoencoder_train_loss_step=8.83e-5, autoencoder_val_loss_step=0.000104, autoencoder_val_loss_epoch=0.000111, autoencoder_train_loss_epoch=0.00012]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 36/36 [30:22<00:00,  0.02it/s, v_num=r6pe, autoencoder_train_loss_step=8.83e-5, autoencoder_val_loss_step=0.000104, autoencoder_val_loss_epoch=0.000111, autoencoder_train_loss_epoch=0.00012]\n",
      "Encoder weights saved to encoder_weights.pth\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>autoencoder_train_loss_epoch</td><td>█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>autoencoder_train_loss_step</td><td>█▃▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>autoencoder_val_loss_epoch</td><td>█▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>autoencoder_val_loss_step</td><td>▆█▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▇▇▇▇▇████</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▂▁▁▂▁▁▁▃▁▂▂▂▂▂▂▂▅▅▅▂▂▆▂▂▂▂▂▂▂▇▂███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>autoencoder_train_loss_epoch</td><td>0.00012</td></tr><tr><td>autoencoder_train_loss_step</td><td>8e-05</td></tr><tr><td>autoencoder_val_loss_epoch</td><td>0.00011</td></tr><tr><td>autoencoder_val_loss_step</td><td>0.0001</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>trainer/global_step</td><td>719</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">earnest-armadillo-12</strong> at: <a href='https://wandb.ai/iaredescnncnn-instituto-tecnol-gico-de-costa-rica/project-three-autoencoder/runs/n89vr6pe' target=\"_blank\">https://wandb.ai/iaredescnncnn-instituto-tecnol-gico-de-costa-rica/project-three-autoencoder/runs/n89vr6pe</a><br/> View project at: <a href='https://wandb.ai/iaredescnncnn-instituto-tecnol-gico-de-costa-rica/project-three-autoencoder' target=\"_blank\">https://wandb.ai/iaredescnncnn-instituto-tecnol-gico-de-costa-rica/project-three-autoencoder</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241115_160912-n89vr6pe\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import hydra\n",
    "from Config.config import Configuration\n",
    "from AutoencoderU import AutoEncoder\n",
    "import pytorch_lightning as L\n",
    "from DataModule import ButterflyDataModule\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "\n",
    "notebook_dir = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "config_dir = os.path.join(notebook_dir,\"proyecto-transfer-learning\\\\Config\")\n",
    "print(config_dir)\n",
    "\n",
    "sys.argv = [arg for arg in sys.argv if not arg.startswith(\"--\")]\n",
    "\n",
    "@hydra.main(config_path=config_dir, config_name=\"config\", version_base=None)\n",
    "def trainining_autoencoder_ninety_10(config: Configuration):\n",
    "    data_manager = ButterflyDataModule(config.DATASET.DATA_DIR,config.TRAIN.BATCH_SIZE, config.SPLIT.NEEDS_SPLIT, \n",
    "    config.SPLIT.SPLIT_RATIO,config.TRAIN.NUM_WORKERS)\n",
    "    data_manager.setup()\n",
    "    wandb_logger = WandbLogger(\n",
    "        project=\"project-three-autoencoder\",\n",
    "        log_model=True,\n",
    "    )\n",
    "    autoencoder = AutoEncoder(config.TRAIN.LEARNING_RATE,config.MODEL.LATENT_DIM)\n",
    "    print(type(autoencoder)) \n",
    "    trainer = L.Trainer(max_epochs=config.TRAIN.NUM_EPOCHS, accelerator=config.TRAIN.ACCELERATION,\n",
    "    precision=config.TRAIN.PRECISION,callbacks=[EarlyStopping(monitor=\"autoencoder_val_loss\", mode=\"min\")], \n",
    "    logger=wandb_logger)\n",
    "    trainer.fit(autoencoder,train_dataloaders=data_manager.unsupervised_train_loader(), \n",
    "    val_dataloaders=data_manager.test_dataloader())\n",
    "\n",
    "    wandb_logger.experiment.finish()\n",
    "\n",
    "#trainining_autoencoder_ninety_10()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@hydra.main(config_path=config_dir, config_name=\"config2\", version_base=None)\n",
    "def trainining_autoencoder_seventy_30(config: Configuration):\n",
    "    data_manager = ButterflyDataModule(config.DATASET.DATA_DIR,config.TRAIN.BATCH_SIZE, config.SPLIT.NEEDS_SPLIT, \n",
    "    config.SPLIT.SPLIT_RATIO,config.TRAIN.NUM_WORKERS)\n",
    "    data_manager.setup()\n",
    "    wandb_logger = WandbLogger(\n",
    "        project=\"project-three-autoencoder\",\n",
    "        log_model=True,\n",
    "    )\n",
    "    autoencoder = AutoEncoder(config.TRAIN.LEARNING_RATE,config.MODEL.LATENT_DIM)\n",
    "    print(type(autoencoder)) \n",
    "    trainer = L.Trainer(max_epochs=config.TRAIN.NUM_EPOCHS, accelerator=config.TRAIN.ACCELERATION,\n",
    "    precision=config.TRAIN.PRECISION,callbacks=[EarlyStopping(monitor=\"autoencoder_val_loss\", mode=\"min\")], \n",
    "    logger=wandb_logger)\n",
    "    trainer.fit(autoencoder,train_dataloaders=data_manager.unsupervised_train_loader(), \n",
    "    val_dataloaders=data_manager.test_dataloader())\n",
    "\n",
    "    wandb_logger.experiment.finish()\n",
    "\n",
    "trainining_autoencoder_seventy_30()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
