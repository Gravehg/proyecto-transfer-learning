{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: r29leonc (iaredescnncnn-instituto-tecnol-gico-de-costa-rica). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\richa\\OneDrive\\Desktop\\TEC\\proyecto-transfer-learning\\Config\n",
      "<class 'AutoencoderU.AutoEncoder'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20241115_172822-exehtxxo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/iaredescnncnn-instituto-tecnol-gico-de-costa-rica/project-three-autoencoder/runs/exehtxxo' target=\"_blank\">scarlet-forest-13</a></strong> to <a href='https://wandb.ai/iaredescnncnn-instituto-tecnol-gico-de-costa-rica/project-three-autoencoder' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/iaredescnncnn-instituto-tecnol-gico-de-costa-rica/project-three-autoencoder' target=\"_blank\">https://wandb.ai/iaredescnncnn-instituto-tecnol-gico-de-costa-rica/project-three-autoencoder</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/iaredescnncnn-instituto-tecnol-gico-de-costa-rica/project-three-autoencoder/runs/exehtxxo' target=\"_blank\">https://wandb.ai/iaredescnncnn-instituto-tecnol-gico-de-costa-rica/project-three-autoencoder/runs/exehtxxo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type    | Params | Mode \n",
      "--------------------------------------------\n",
      "0 | encoder | Encoder | 4.7 M  | train\n",
      "1 | decoder | Decoder | 3.0 M  | train\n",
      "2 | loss_fn | MSELoss | 0      | train\n",
      "--------------------------------------------\n",
      "7.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.7 M     Total params\n",
      "30.790    Total estimated model params size (MB)\n",
      "42        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "368a865b7c5145278c8a755ab09e58a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\richa\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:419: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n",
      "c:\\Users\\richa\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:419: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n",
      "c:\\Users\\richa\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:298: The number of training batches (36) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fed00a543ec4e4ab86b9ed2710b4f67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb8be403554646b3be5268a598ebe9c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b39ebaf519314a8e8773943807c68daa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66f88dcbf4e9494593a42e9287375da0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eccd044fc18046a58a0985f97870acf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error executing job with overrides: []\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\richa\\AppData\\Local\\Temp\\ipykernel_20340\\753413832.py\", line 34, in trainining_autoencoder_ninety_10\n",
      "    trainer.fit(autoencoder,train_dataloaders=data_manager.unsupervised_train_loader(),\n",
      "  File \"c:\\Users\\richa\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 538, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"c:\\Users\\richa\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\richa\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 574, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"c:\\Users\\richa\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 981, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\richa\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 1025, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"c:\\Users\\richa\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py\", line 205, in run\n",
      "    self.advance()\n",
      "  File \"c:\\Users\\richa\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py\", line 363, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"c:\\Users\\richa\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py\", line 140, in run\n",
      "    self.advance(data_fetcher)\n",
      "  File \"c:\\Users\\richa\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py\", line 212, in advance\n",
      "    batch, _, __ = next(data_fetcher)\n",
      "                   ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\richa\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\loops\\fetchers.py\", line 133, in __next__\n",
      "    batch = super().__next__()\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\richa\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\loops\\fetchers.py\", line 60, in __next__\n",
      "    batch = next(self.iterator)\n",
      "            ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\richa\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\utilities\\combined_loader.py\", line 341, in __next__\n",
      "    out = next(self._iterator)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\richa\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\utilities\\combined_loader.py\", line 78, in __next__\n",
      "    out[i] = next(self.iterators[i])\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\richa\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 630, in __next__\n",
      "    data = self._next_data()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\richa\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1344, in _next_data\n",
      "    return self._process_data(data)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\richa\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1370, in _process_data\n",
      "    data.reraise()\n",
      "  File \"c:\\Users\\richa\\anaconda3\\Lib\\site-packages\\torch\\_utils.py\", line 706, in reraise\n",
      "    raise exception\n",
      "RuntimeError: Caught RuntimeError in DataLoader worker process 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"c:\\Users\\richa\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 309, in _worker_loop\n",
      "    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\richa\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 55, in fetch\n",
      "    return self.collate_fn(data)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\richa\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 317, in default_collate\n",
      "    return collate(batch, collate_fn_map=default_collate_fn_map)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\richa\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 174, in collate\n",
      "    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\richa\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 142, in collate\n",
      "    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\richa\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 212, in collate_tensor_fn\n",
      "    storage = elem._typed_storage()._new_shared(numel, device=elem.device)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\richa\\anaconda3\\Lib\\site-packages\\torch\\storage.py\", line 968, in _new_shared\n",
      "    untyped_storage = torch.UntypedStorage._new_shared(size * self._element_size(), device=device)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\richa\\anaconda3\\Lib\\site-packages\\torch\\storage.py\", line 300, in _new_shared\n",
      "    return cls._new_using_filename_cpu(size)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: Couldn't open shared file mapping: <torch_17960_1826528471_0>, error code: <1455>\n",
      "\n",
      "\n",
      "Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'tb_frame'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\hydra\\_internal\\utils.py:220\u001b[0m, in \u001b[0;36mrun_and_report\u001b[1;34m(func)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func()\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\hydra\\_internal\\utils.py:458\u001b[0m, in \u001b[0;36m_run_app.<locals>.<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m RunMode\u001b[38;5;241m.\u001b[39mRUN:\n\u001b[0;32m    457\u001b[0m     run_and_report(\n\u001b[1;32m--> 458\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: hydra\u001b[38;5;241m.\u001b[39mrun(\n\u001b[0;32m    459\u001b[0m             config_name\u001b[38;5;241m=\u001b[39mconfig_name,\n\u001b[0;32m    460\u001b[0m             task_function\u001b[38;5;241m=\u001b[39mtask_function,\n\u001b[0;32m    461\u001b[0m             overrides\u001b[38;5;241m=\u001b[39moverrides,\n\u001b[0;32m    462\u001b[0m         )\n\u001b[0;32m    463\u001b[0m     )\n\u001b[0;32m    464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\hydra\\_internal\\hydra.py:132\u001b[0m, in \u001b[0;36mHydra.run\u001b[1;34m(self, config_name, task_function, overrides, with_log_configuration)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;66;03m# access the result to trigger an exception in case the job failed.\u001b[39;00m\n\u001b[1;32m--> 132\u001b[0m _ \u001b[38;5;241m=\u001b[39m ret\u001b[38;5;241m.\u001b[39mreturn_value\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\hydra\\core\\utils.py:260\u001b[0m, in \u001b[0;36mJobReturn.return_value\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    257\u001b[0m sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(\n\u001b[0;32m    258\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError executing job with overrides: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverrides\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m os\u001b[38;5;241m.\u001b[39mlinesep\n\u001b[0;32m    259\u001b[0m )\n\u001b[1;32m--> 260\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_value\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\hydra\\core\\utils.py:186\u001b[0m, in \u001b[0;36mrun_job\u001b[1;34m(task_function, config, job_dir_key, job_subdir_key, hydra_context, configure_logging)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 186\u001b[0m     ret\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m task_function(task_cfg)\n\u001b[0;32m    187\u001b[0m     ret\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m JobStatus\u001b[38;5;241m.\u001b[39mCOMPLETED\n",
      "Cell \u001b[1;32mIn[1], line 34\u001b[0m, in \u001b[0;36mtrainining_autoencoder_ninety_10\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m     31\u001b[0m trainer \u001b[38;5;241m=\u001b[39m L\u001b[38;5;241m.\u001b[39mTrainer(max_epochs\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mTRAIN\u001b[38;5;241m.\u001b[39mNUM_EPOCHS, accelerator\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mTRAIN\u001b[38;5;241m.\u001b[39mACCELERATION,\n\u001b[0;32m     32\u001b[0m precision\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mTRAIN\u001b[38;5;241m.\u001b[39mPRECISION,callbacks\u001b[38;5;241m=\u001b[39m[EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautoencoder_val_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m)], \n\u001b[0;32m     33\u001b[0m logger\u001b[38;5;241m=\u001b[39mwandb_logger)\n\u001b[1;32m---> 34\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(autoencoder,train_dataloaders\u001b[38;5;241m=\u001b[39mdata_manager\u001b[38;5;241m.\u001b[39munsupervised_train_loader(), \n\u001b[0;32m     35\u001b[0m val_dataloaders\u001b[38;5;241m=\u001b[39mdata_manager\u001b[38;5;241m.\u001b[39mtest_dataloader())\n\u001b[0;32m     37\u001b[0m wandb_logger\u001b[38;5;241m.\u001b[39mexperiment\u001b[38;5;241m.\u001b[39mfinish()\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 538\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_and_handle_interrupt(\n\u001b[0;32m    539\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[0;32m    540\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:574\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    568\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[0;32m    570\u001b[0m     ckpt_path,\n\u001b[0;32m    571\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    572\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    573\u001b[0m )\n\u001b[1;32m--> 574\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(model, ckpt_path\u001b[38;5;241m=\u001b[39mckpt_path)\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    978\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    979\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 981\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_stage()\n\u001b[0;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1025\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m-> 1025\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[1;32m--> 205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance()\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 363\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch_loop\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher)\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:140\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance(data_fetcher)\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:212\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    211\u001b[0m dataloader_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 212\u001b[0m batch, _, __ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(data_fetcher)\n\u001b[0;32m    213\u001b[0m \u001b[38;5;66;03m# TODO: we should instead use the batch_idx returned by the fetcher, however, that will require saving the\u001b[39;00m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;66;03m# fetcher state so that the batch_idx is correct after restarting\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\loops\\fetchers.py:133\u001b[0m, in \u001b[0;36m_PrefetchDataFetcher.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;66;03m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__next__\u001b[39m()\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;66;03m# the iterator is empty\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\loops\\fetchers.py:60\u001b[0m, in \u001b[0;36m_DataFetcher.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 60\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\utilities\\combined_loader.py:341\u001b[0m, in \u001b[0;36mCombinedLoader.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 341\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator)\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator, _Sequential):\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\utilities\\combined_loader.py:78\u001b[0m, in \u001b[0;36m_MaxSizeCycle.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 78\u001b[0m     out[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterators[i])\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1344\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1343\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[1;32m-> 1344\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1370\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[1;32m-> 1370\u001b[0m     data\u001b[38;5;241m.\u001b[39mreraise()\n\u001b[0;32m   1371\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\torch\\_utils.py:706\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 706\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"c:\\Users\\richa\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 309, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\richa\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\richa\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 317, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\richa\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 174, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\richa\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 142, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\richa\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 212, in collate_tensor_fn\n    storage = elem._typed_storage()._new_shared(numel, device=elem.device)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\richa\\anaconda3\\Lib\\site-packages\\torch\\storage.py\", line 968, in _new_shared\n    untyped_storage = torch.UntypedStorage._new_shared(size * self._element_size(), device=device)\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\richa\\anaconda3\\Lib\\site-packages\\torch\\storage.py\", line 300, in _new_shared\n    return cls._new_using_filename_cpu(size)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Couldn't open shared file mapping: <torch_17960_1826528471_0>, error code: <1455>\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[1], line 39\u001b[0m\n\u001b[0;32m     37\u001b[0m     wandb_logger\u001b[38;5;241m.\u001b[39mexperiment\u001b[38;5;241m.\u001b[39mfinish()\n\u001b[1;32m---> 39\u001b[0m trainining_autoencoder_ninety_10()\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\hydra\\main.py:94\u001b[0m, in \u001b[0;36mmain.<locals>.main_decorator.<locals>.decorated_main\u001b[1;34m(cfg_passthrough)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;66;03m# no return value from run_hydra() as it may sometime actually run the task_function\u001b[39;00m\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;66;03m# multiple times (--multirun)\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m     _run_hydra(\n\u001b[0;32m     95\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m     96\u001b[0m         args_parser\u001b[38;5;241m=\u001b[39margs_parser,\n\u001b[0;32m     97\u001b[0m         task_function\u001b[38;5;241m=\u001b[39mtask_function,\n\u001b[0;32m     98\u001b[0m         config_path\u001b[38;5;241m=\u001b[39mconfig_path,\n\u001b[0;32m     99\u001b[0m         config_name\u001b[38;5;241m=\u001b[39mconfig_name,\n\u001b[0;32m    100\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\hydra\\_internal\\utils.py:394\u001b[0m, in \u001b[0;36m_run_hydra\u001b[1;34m(args, args_parser, task_function, config_path, config_name, caller_stack_depth)\u001b[0m\n\u001b[0;32m    393\u001b[0m     run_mode \u001b[38;5;241m=\u001b[39m hydra\u001b[38;5;241m.\u001b[39mget_mode(config_name\u001b[38;5;241m=\u001b[39mconfig_name, overrides\u001b[38;5;241m=\u001b[39moverrides)\n\u001b[1;32m--> 394\u001b[0m     _run_app(\n\u001b[0;32m    395\u001b[0m         run\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mrun,\n\u001b[0;32m    396\u001b[0m         multirun\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mmultirun,\n\u001b[0;32m    397\u001b[0m         mode\u001b[38;5;241m=\u001b[39mrun_mode,\n\u001b[0;32m    398\u001b[0m         hydra\u001b[38;5;241m=\u001b[39mhydra,\n\u001b[0;32m    399\u001b[0m         config_name\u001b[38;5;241m=\u001b[39mconfig_name,\n\u001b[0;32m    400\u001b[0m         task_function\u001b[38;5;241m=\u001b[39mtask_function,\n\u001b[0;32m    401\u001b[0m         overrides\u001b[38;5;241m=\u001b[39moverrides,\n\u001b[0;32m    402\u001b[0m     )\n\u001b[0;32m    403\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mcfg:\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\hydra\\_internal\\utils.py:457\u001b[0m, in \u001b[0;36m_run_app\u001b[1;34m(run, multirun, mode, hydra, config_name, task_function, overrides)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m RunMode\u001b[38;5;241m.\u001b[39mRUN:\n\u001b[1;32m--> 457\u001b[0m     run_and_report(\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: hydra\u001b[38;5;241m.\u001b[39mrun(\n\u001b[0;32m    459\u001b[0m             config_name\u001b[38;5;241m=\u001b[39mconfig_name,\n\u001b[0;32m    460\u001b[0m             task_function\u001b[38;5;241m=\u001b[39mtask_function,\n\u001b[0;32m    461\u001b[0m             overrides\u001b[38;5;241m=\u001b[39moverrides,\n\u001b[0;32m    462\u001b[0m         )\n\u001b[0;32m    463\u001b[0m     )\n\u001b[0;32m    464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\hydra\\_internal\\utils.py:303\u001b[0m, in \u001b[0;36mrun_and_report\u001b[1;34m(func)\u001b[0m\n\u001b[0;32m    302\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ex\n\u001b[1;32m--> 303\u001b[0m sys\u001b[38;5;241m.\u001b[39mexit(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\wandb\\sdk\\lib\\exit_hooks.py:36\u001b[0m, in \u001b[0;36mExitHooks.exit\u001b[1;34m(self, code)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexit_code \u001b[38;5;241m=\u001b[39m code\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_orig_exit(orig_code)\n",
      "\u001b[1;31mSystemExit\u001b[0m: 1",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:2145\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exception_only:\n\u001b[0;32m   2143\u001b[0m     stb \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAn exception has occurred, use \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mtb to see \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   2144\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe full traceback.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m-> 2145\u001b[0m     stb\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mInteractiveTB\u001b[38;5;241m.\u001b[39mget_exception_only(etype,\n\u001b[0;32m   2146\u001b[0m                                                      value))\n\u001b[0;32m   2147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2149\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontains_exceptiongroup\u001b[39m(val):\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py:710\u001b[0m, in \u001b[0;36mListTB.get_exception_only\u001b[1;34m(self, etype, value)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_exception_only\u001b[39m(\u001b[38;5;28mself\u001b[39m, etype, value):\n\u001b[0;32m    703\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Only print the exception type and message, without a traceback.\u001b[39;00m\n\u001b[0;32m    704\u001b[0m \n\u001b[0;32m    705\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    708\u001b[0m \u001b[38;5;124;03m    value : exception value\u001b[39;00m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ListTB\u001b[38;5;241m.\u001b[39mstructured_traceback(\u001b[38;5;28mself\u001b[39m, etype, value)\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py:568\u001b[0m, in \u001b[0;36mListTB.structured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[0;32m    565\u001b[0m     chained_exc_ids\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mid\u001b[39m(exception[\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m    566\u001b[0m     chained_exceptions_tb_offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    567\u001b[0m     out_list \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 568\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstructured_traceback(\n\u001b[0;32m    569\u001b[0m             etype,\n\u001b[0;32m    570\u001b[0m             evalue,\n\u001b[0;32m    571\u001b[0m             (etb, chained_exc_ids),  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    572\u001b[0m             chained_exceptions_tb_offset,\n\u001b[0;32m    573\u001b[0m             context,\n\u001b[0;32m    574\u001b[0m         )\n\u001b[0;32m    575\u001b[0m         \u001b[38;5;241m+\u001b[39m chained_exception_message\n\u001b[0;32m    576\u001b[0m         \u001b[38;5;241m+\u001b[39m out_list)\n\u001b[0;32m    578\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out_list\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py:1454\u001b[0m, in \u001b[0;36mAutoFormattedTB.structured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1452\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1453\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtb \u001b[38;5;241m=\u001b[39m etb\n\u001b[1;32m-> 1454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m FormattedTB\u001b[38;5;241m.\u001b[39mstructured_traceback(\n\u001b[0;32m   1455\u001b[0m     \u001b[38;5;28mself\u001b[39m, etype, evalue, etb, tb_offset, number_of_lines_of_context\n\u001b[0;32m   1456\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py:1345\u001b[0m, in \u001b[0;36mFormattedTB.structured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1342\u001b[0m mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\n\u001b[0;32m   1343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose_modes:\n\u001b[0;32m   1344\u001b[0m     \u001b[38;5;66;03m# Verbose modes need a full traceback\u001b[39;00m\n\u001b[1;32m-> 1345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m VerboseTB\u001b[38;5;241m.\u001b[39mstructured_traceback(\n\u001b[0;32m   1346\u001b[0m         \u001b[38;5;28mself\u001b[39m, etype, value, tb, tb_offset, number_of_lines_of_context\n\u001b[0;32m   1347\u001b[0m     )\n\u001b[0;32m   1348\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMinimal\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m   1349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ListTB\u001b[38;5;241m.\u001b[39mget_exception_only(\u001b[38;5;28mself\u001b[39m, etype, value)\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py:1192\u001b[0m, in \u001b[0;36mVerboseTB.structured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstructured_traceback\u001b[39m(\n\u001b[0;32m   1184\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1185\u001b[0m     etype: \u001b[38;5;28mtype\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1189\u001b[0m     number_of_lines_of_context: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m   1190\u001b[0m ):\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1192\u001b[0m     formatted_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0;32m   1193\u001b[0m                                                            tb_offset)\n\u001b[0;32m   1195\u001b[0m     colors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mColors  \u001b[38;5;66;03m# just a shorthand + quicker name lookup\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m     colorsnormal \u001b[38;5;241m=\u001b[39m colors\u001b[38;5;241m.\u001b[39mNormal  \u001b[38;5;66;03m# used a lot\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py:1082\u001b[0m, in \u001b[0;36mVerboseTB.format_exception_as_a_whole\u001b[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1079\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tb_offset, \u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m   1080\u001b[0m head \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_header(\u001b[38;5;28mstr\u001b[39m(etype), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlong_header)\n\u001b[0;32m   1081\u001b[0m records \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1082\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_records(etb, number_of_lines_of_context, tb_offset) \u001b[38;5;28;01mif\u001b[39;00m etb \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[0;32m   1083\u001b[0m )\n\u001b[0;32m   1085\u001b[0m frames \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   1086\u001b[0m skipped \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\richa\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py:1150\u001b[0m, in \u001b[0;36mVerboseTB.get_records\u001b[1;34m(self, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1148\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m cf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1149\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1150\u001b[0m         mod \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39mgetmodule(cf\u001b[38;5;241m.\u001b[39mtb_frame)\n\u001b[0;32m   1151\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mod \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1152\u001b[0m             mod_name \u001b[38;5;241m=\u001b[39m mod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'tb_frame'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import hydra\n",
    "from Config.config import Configuration\n",
    "from AutoencoderU import AutoEncoder\n",
    "import pytorch_lightning as L\n",
    "from DataModule import ButterflyDataModule\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "notebook_dir = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "config_dir = os.path.join(notebook_dir,\"proyecto-transfer-learning\\\\Config\")\n",
    "print(config_dir)\n",
    "\n",
    "sys.argv = [arg for arg in sys.argv if not arg.startswith(\"--\")]\n",
    "\n",
    "@hydra.main(config_path=config_dir, config_name=\"config\", version_base=None)\n",
    "def trainining_autoencoder_ninety_10(config: Configuration):\n",
    "    data_manager = ButterflyDataModule(config.DATASET.DATA_DIR,config.TRAIN.BATCH_SIZE, config.SPLIT.NEEDS_SPLIT, \n",
    "    config.SPLIT.SPLIT_RATIO,config.TRAIN.NUM_WORKERS)\n",
    "    data_manager.setup()\n",
    "    wandb_logger = WandbLogger(\n",
    "        project=\"project-three-autoencoder\",\n",
    "        log_model=True,\n",
    "    )\n",
    "    autoencoder = AutoEncoder(config.TRAIN.LEARNING_RATE,config.MODEL.LATENT_DIM)\n",
    "    print(type(autoencoder)) \n",
    "    trainer = L.Trainer(max_epochs=config.TRAIN.NUM_EPOCHS, accelerator=config.TRAIN.ACCELERATION,\n",
    "    precision=config.TRAIN.PRECISION,callbacks=[EarlyStopping(monitor=\"autoencoder_val_loss\", mode=\"min\")], \n",
    "    logger=wandb_logger)\n",
    "    trainer.fit(autoencoder,train_dataloaders=data_manager.unsupervised_train_loader(), \n",
    "    val_dataloaders=data_manager.test_dataloader())\n",
    "\n",
    "    wandb_logger.experiment.finish()\n",
    "\n",
    "trainining_autoencoder_ninety_10()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
